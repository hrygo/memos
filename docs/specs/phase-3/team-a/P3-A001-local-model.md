# P3-A001: æœ¬åœ°æ¨¡å‹é›†æˆ

> **çŠ¶æ€**: ğŸ”² å¾…å¼€å‘  
> **ä¼˜å…ˆçº§**: P2 (å¢å¼º)  
> **æŠ•å…¥**: 5 äººå¤©  
> **è´Ÿè´£å›¢é˜Ÿ**: å›¢é˜Ÿ A  
> **Sprint**: Sprint 5

---

## 1. ç›®æ ‡ä¸èƒŒæ™¯

### 1.1 æ ¸å¿ƒç›®æ ‡

é›†æˆæœ¬åœ° LLMï¼ˆOllama/llama.cppï¼‰ï¼Œæ”¯æŒç¦»çº¿è¿è¡Œå’Œéšç§æ•æ„Ÿåœºæ™¯ï¼Œé™ä½ API æˆæœ¬ 80%+ã€‚

### 1.2 ç”¨æˆ·ä»·å€¼

- å®Œå…¨ç¦»çº¿ä½¿ç”¨
- æ•°æ®ä¸å‡ºæœ¬åœ°
- API æˆæœ¬å½’é›¶ï¼ˆæœ¬åœ°æ¨ç†ï¼‰

### 1.3 æŠ€æœ¯ä»·å€¼

- é™ä½äº‘ç«¯ä¾èµ–
- æ”¯æŒç§æœ‰éƒ¨ç½²
- ä¸ºæ¨¡å‹è·¯ç”±ï¼ˆP3-A002ï¼‰å¥ å®šåŸºç¡€

---

## 2. ä¾èµ–å…³ç³»

### 2.1 å‰ç½®ä¾èµ–

- [x] P1-A003: LLM è·¯ç”±ä¼˜åŒ–ï¼ˆè·¯ç”±åŸºç¡€ï¼‰

### 2.2 åç»­ä¾èµ–

- P3-A002: æ¨¡å‹è·¯ç”±å™¨ï¼ˆæœ¬åœ°/äº‘ç«¯åˆ‡æ¢ï¼‰

---

## 3. åŠŸèƒ½è®¾è®¡

### 3.1 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æœ¬åœ°æ¨¡å‹é›†æˆæ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚              LocalLLMProvider                        â”‚ â”‚
â”‚   â”‚                                                      â”‚ â”‚
â”‚   â”‚  æ”¯æŒåç«¯:                                           â”‚ â”‚
â”‚   â”‚  â”œâ”€ Ollama (æ¨èï¼Œæ˜“éƒ¨ç½²)                           â”‚ â”‚
â”‚   â”‚  â”œâ”€ llama.cpp (è½»é‡çº§)                              â”‚ â”‚
â”‚   â”‚  â””â”€ vLLM (é«˜æ€§èƒ½)                                   â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚                               â”‚
â”‚                            â–¼                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚              æ¨èæ¨¡å‹                                â”‚ â”‚
â”‚   â”‚                                                      â”‚ â”‚
â”‚   â”‚  â€¢ Qwen2.5-7B-Instruct (ä¸­æ–‡æœ€ä¼˜)                   â”‚ â”‚
â”‚   â”‚  â€¢ Llama-3.2-3B (è½»é‡)                              â”‚ â”‚
â”‚   â”‚  â€¢ Mistral-7B (é€šç”¨)                                â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                            â”‚
â”‚   ç¡¬ä»¶è¦æ±‚: 8GB+ RAM (7Bæ¨¡å‹) | 16GB+ (13Bæ¨¡å‹)           â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ ¸å¿ƒæ¥å£

```go
// plugin/ai/llm/local_provider.go

type LocalLLMProvider interface {
    LLMProvider
    
    // æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å¯ç”¨
    IsAvailable(ctx context.Context) bool
    
    // è·å–å·²å®‰è£…æ¨¡å‹åˆ—è¡¨
    ListModels(ctx context.Context) ([]LocalModel, error)
    
    // æ‹‰å–æ¨¡å‹
    PullModel(ctx context.Context, modelName string) error
}

type LocalModel struct {
    Name      string `json:"name"`
    Size      int64  `json:"size"`
    Quantization string `json:"quantization"`  // q4_0, q8_0, f16
}
```

### 3.3 Ollama é›†æˆ

```go
// plugin/ai/llm/ollama.go

type OllamaProvider struct {
    baseURL string
    timeout time.Duration
}

func NewOllamaProvider(baseURL string) *OllamaProvider {
    if baseURL == "" {
        baseURL = "http://localhost:11434"
    }
    return &OllamaProvider{
        baseURL: baseURL,
        timeout: 60 * time.Second,
    }
}

func (p *OllamaProvider) Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
    payload := map[string]any{
        "model":  req.Model,
        "prompt": req.Prompt,
        "stream": false,
        "options": map[string]any{
            "temperature": req.Temperature,
            "num_predict": req.MaxTokens,
        },
    }
    
    resp, err := p.post(ctx, "/api/generate", payload)
    if err != nil {
        return nil, err
    }
    
    return &CompletionResponse{
        Content: resp["response"].(string),
        Model:   req.Model,
        Usage: TokenUsage{
            PromptTokens:     resp["prompt_eval_count"].(int),
            CompletionTokens: resp["eval_count"].(int),
        },
    }, nil
}

func (p *OllamaProvider) IsAvailable(ctx context.Context) bool {
    resp, err := http.Get(p.baseURL + "/api/tags")
    return err == nil && resp.StatusCode == 200
}
```

### 3.4 é…ç½®

```yaml
# configs/ai.yaml
local_llm:
  enabled: true
  provider: "ollama"  # ollama, llamacpp, vllm
  
  ollama:
    base_url: "http://localhost:11434"
    default_model: "qwen2.5:7b"
    timeout: 60s
    
  models:
    chat: "qwen2.5:7b"
    embedding: "nomic-embed-text"
```

---

## 4. å®ç°è·¯å¾„

| Day | ä»»åŠ¡ |
|-----|------|
| 1-2 | Ollama Provider å®ç° |
| 3 | æ¨¡å‹ç®¡ç†ï¼ˆåˆ—è¡¨ã€æ‹‰å–ï¼‰ |
| 4 | Embedding æ”¯æŒ |
| 5 | æµ‹è¯•ä¸æ–‡æ¡£ |

---

## 5. éªŒæ”¶æ ‡å‡†

- [ ] Ollama å¯ç”¨æ—¶è‡ªåŠ¨æ£€æµ‹
- [ ] æœ¬åœ°æ¨ç†å»¶è¿Ÿ < 5sï¼ˆ7Bæ¨¡å‹ï¼‰
- [ ] æ”¯æŒ Qwen2.5-7B ä¸­æ–‡å¯¹è¯

---

> **ç‰ˆæœ¬**: v1.0 | **æ›´æ–°æ—¶é—´**: 2026-01-27
